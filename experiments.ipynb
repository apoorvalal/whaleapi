{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `WhaleAPI`: A class to programmatically call Deepseek API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# %pip install pdir2 - better object introspection\n",
    "import pdir\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This populates the API key as `DEEPSEEK_API_KEY` in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api import WhaleAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = WhaleAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deepseek-chat', 'deepseek-reasoner']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "property:\n",
       "    api_key, headers\n",
       "special attribute:\n",
       "    __class__, __dict__, __doc__, __module__, __weakref__\n",
       "abstract class:\n",
       "    __subclasshook__\n",
       "object customization:\n",
       "    __format__, __hash__, __init__, __new__, __repr__, __sizeof__, __str__\n",
       "rich comparison:\n",
       "    __eq__, __ge__, __gt__, __le__, __lt__, __ne__\n",
       "attribute access:\n",
       "    __delattr__, __dir__, __getattribute__, __setattr__\n",
       "class customization:\n",
       "    __init_subclass__\n",
       "pickle:\n",
       "    __reduce__, __reduce_ex__\n",
       "function:\n",
       "    _completion_impl: Internal method to handle streaming completions.\n",
       "    _post_request: Internal method to handle POST requests.\n",
       "    chat_completion: Generate a chat completion.\n",
       "    fim_completion: Generate a fill-in-the-middle (FIM) completion.\n",
       "    get_models: Fetch available models.\n",
       "    user_balance: Fetch the user's balance."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdir(api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chat completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a user prompt\n",
    "user_prompt = \"What is the capital of Indonesia?\"\n",
    "# Get a chat completion\n",
    "response = api.chat_completion(prompt=user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The capital of Indonesia is **Jakarta**. However, Indonesia is planning to move its capital to **Nusantara**, located in East Kalimantan on the island of Borneo, with the relocation expected to begin in 2024."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### streaming completion\n",
    "(prints the completion as it comes in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Response:\n",
      "Endogeneity in regression occurs when an independent variable (predictor) is correlated with the error term in the model. This violates a key assumption of regression analysis, leading to biased and inconsistent estimates of the coefficients. Endogeneity can arise due to:\n",
      "\n",
      "1. **Omitted Variable Bias**: A relevant variable is left out of the model, and its effect is captured in the error term.\n",
      "2. **Measurement Error**: The independent variable is measured inaccurately, causing it to correlate with the error term.\n",
      "3. **Simultaneity**: The dependent and independent variables influence each other simultaneously (e.g., supply and demand).\n",
      "4. **Selection Bias**: The sample is not randomly selected, leading to a correlation between the error term and the independent variable.\n",
      "\n",
      "### Popular Methods to Solve Endogeneity:\n",
      "\n",
      "1. **Instrumental Variables (IV)**:\n",
      "   - Use an \"instrument\" (a variable correlated with the endogenous independent variable but not with the error term) to isolate the variation in the independent variable that is not correlated with the error term.\n",
      "   - Example: Using rainfall as an instrument for agricultural productivity in a study on income.\n",
      "\n",
      "2. **Control Variables**:\n",
      "   - Include additional variables in the model to account for omitted factors that might cause endogeneity.\n",
      "\n",
      "3. **Fixed Effects Models**:\n",
      "   - Use panel data to control for unobserved, time-invariant characteristics that might be causing endogeneity.\n",
      "\n",
      "4. **Difference-in-Differences (DiD)**:\n",
      "   - Compare changes over time between a treatment group and a control group to account for unobserved confounders.\n",
      "\n",
      "5. **Regression Discontinuity Design (RDD)**:\n",
      "   - Exploit a cutoff or threshold in the assignment of treatment to estimate causal effects.\n",
      "\n",
      "6. **Propensity Score Matching**:\n",
      "   - Match treated and untreated observations with similar characteristics to reduce selection bias.\n",
      "\n",
      "These methods aim to break the correlation between the independent variable and the error term, ensuring more reliable regression results."
     ]
    }
   ],
   "source": [
    "# Define a user prompt\n",
    "user_prompt = \"Explain endogeneity in regression and the most popular methods to solve them in simple terms.\"\n",
    "\n",
    "# Get a streaming chat completion\n",
    "stream_response = api.chat_completion(prompt=user_prompt, stream=True)\n",
    "\n",
    "# Print the response in real-time\n",
    "print(\"Streaming Response:\")\n",
    "for chunk in stream_response:\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FiM (fill in the middle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To implement an Ordinary Least Squares (OLS) solver in Python using NumPy, you can use the following function:\n",
       "\n",
       "```python\n",
       "import numpy as np\n",
       "\n",
       "def OLS_solver(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
       "    # Add a column of ones to X to account for the intercept term\n",
       "    X = np.column_stack((np.ones(X.shape[0]), X))\n",
       "    \n",
       "    # Calculate the OLS coefficients using the normal equation\n",
       "    beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
       "    \n",
       "    return beta\n",
       "```\n",
       "\n",
       "### Explanation:\n",
       "1. **Adding a column of ones**: The function first adds a column of ones to the input matrix `X` to account for the intercept term in the linear regression model.\n",
       "\n",
       "2. **Normal equation**: The OLS coefficients are calculated using the normal equation:\n",
       "   \\[\n",
       "   \\beta = (X^T X)^{-1} X^T y\n",
       "   \\]\n",
       "   where:\n",
       "   - \\( X \\) is the design matrix (with the added column of ones),\n",
       "   - \\( y \\) is the target vector,\n",
       "   - \\( \\beta \\) is the vector of coefficients (including the intercept).\n",
       "\n",
       "3. **Returning the coefficients**: The function returns the vector of coefficients `beta`.\n",
       "\n",
       "### Example Usage:\n",
       "```python\n",
       "# Example data\n",
       "X = np.array([[1, 2], [2, 3], [3, 4]])\n",
       "y = np.array([1, 2, 3])\n",
       "\n",
       "# Solve for OLS coefficients\n",
       "coefficients = OLS_solver(X, y)\n",
       "print(coefficients)\n",
       "```\n",
       "\n",
       "This will output the coefficients of the linear regression model, including the intercept.\n",
       "\n",
       "### Notes:\n",
       "- This implementation assumes that `X` is a 2D array where each row represents a sample and each column represents a feature.\n",
       "- The function uses the normal equation, which is straightforward but may not be the most numerically stable or efficient method for very large datasets. For large datasets, consider using iterative methods or libraries like `scikit-learn`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# partial code prompt\n",
    "code_prompt = \"def OLS_solver(X: np.ndarray, y: np.ndarray) -> np.ndarray:\\n return\"\n",
    "\n",
    "# Get a FIM completion\n",
    "response = api.fim_completion(prompt=code_prompt)\n",
    "printmd(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Response (max_tokens=2048):\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The theory of relativity, developed by Albert Einstein, consists of two parts: **Special Relativity** and **General Relativity**.\n",
       "\n",
       "1. **Special Relativity** (1905):  \n",
       "   - It explains how time and space are interconnected and how they behave for objects moving at constant speeds, especially near the speed of light.  \n",
       "   - Key ideas:  \n",
       "     - The laws of physics are the same for all observers in uniform motion (no acceleration).  \n",
       "     - The speed of light is constant and the same for everyone, no matter how fast they're moving.  \n",
       "     - Time slows down (time dilation) and lengths contract (length contraction) for objects moving close to the speed of light.  \n",
       "\n",
       "2. **General Relativity** (1915):  \n",
       "   - It extends these ideas to include gravity, explaining it as the curvature of spacetime caused by mass and energy.  \n",
       "   - Key ideas:  \n",
       "     - Massive objects like planets and stars warp the fabric of spacetime.  \n",
       "     - Objects move along curved paths (orbits) because of this curvature, which we experience as gravity.  \n",
       "\n",
       "In simple terms:  \n",
       "- **Special Relativity** deals with fast-moving objects and the constant speed of light.  \n",
       "- **General Relativity** explains gravity as the bending of spacetime by mass and energy.  \n",
       "\n",
       "Together, they revolutionized our understanding of space, time, and gravity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "api = WhaleAPI()\n",
    "\n",
    "prompt = \"Explain the theory of relativity in simple terms.\"\n",
    "\n",
    "# Example 1: Default payload (max_tokens=2048)\n",
    "default_response = api.chat_completion(prompt=prompt)\n",
    "print(\"Default Response (max_tokens=2048):\")\n",
    "printmd(default_response)\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short Response (max_tokens=100):\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The theory of relativity, developed by Albert Einstein, consists of two parts: **Special Relativity** and **General Relativity**.\n",
       "\n",
       "1. **Special Relativity** (1905):  \n",
       "   - It explains how time and space are interconnected and how they behave for objects moving at constant speeds, especially near the speed of light.  \n",
       "   - Key ideas:  \n",
       "     - The laws of physics are the same for all observers in uniform motion (no acceleration).  \n",
       "     - The speed of light is"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example 2: Short response (max_tokens=50)\n",
    "short_response = api.chat_completion(prompt=prompt, max_tokens=100)\n",
    "print(\"Short Response (max_tokens=100):\")\n",
    "printmd(short_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To demonstrate why minimizing the L1 error yields the **median** and minimizing the L2 error yields the **mean**, consider a dataset \\( y_1, y_2, \\dots, y_n \\) and a scalar \\( m \\) to be estimated.\n",
       "\n",
       "---\n",
       "\n",
       "### **L2 Error (Mean)**\n",
       "The L2 error is the sum of squared residuals:  \n",
       "\\[\n",
       "E_{\\text{L2}} = \\sum_{i=1}^n (y_i - m)^2.\n",
       "\\]  \n",
       "To minimize \\( E_{\\text{L2}} \\), take the derivative with respect to \\( m \\):  \n",
       "\\[\n",
       "\\frac{dE_{\\text{L2}}}{dm} = -2 \\sum_{i=1}^n (y_i - m).\n",
       "\\]  \n",
       "Set the derivative to zero for optimality:  \n",
       "\\[\n",
       "\\sum_{i=1}^n (y_i - m) = 0 \\implies m = \\frac{1}{n} \\sum_{i=1}^n y_i,\n",
       "\\]  \n",
       "which is the **mean**. The L2 penalty disproportionately penalizes large errors, leading to a balance of residuals (summing to zero).\n",
       "\n",
       "---\n",
       "\n",
       "### **L1 Error (Median)**\n",
       "The L1 error is the sum of absolute residuals:  \n",
       "\\[\n",
       "E_{\\text{L1}} = \\sum_{i=1}^n |y_i - m|.\n",
       "\\]  \n",
       "The absolute value function is not smooth at \\( m = y_i \\), so we analyze subderivatives. The derivative of \\( |y_i - m| \\) with respect to \\( m \\) is:  \n",
       "\\[\n",
       "\\frac{d}{dm}|y_i - m| = \n",
       "\\begin{cases} \n",
       "-1 & \\text{if } m < y_i, \\\\\n",
       "+1 & \\text{if } m > y_i, \\\\\n",
       "\\text{undefined} & \\text{if } m = y_i.\n",
       "\\end{cases}\n",
       "\\]  \n",
       "The optimal \\( m \\) occurs when the number of positive and negative derivatives balance. Let \\( m^* \\) be the median. For \\( m < m^* \\), more residuals have \\( m < y_i \\), so the derivative is negative (pushing \\( m \\) up). For \\( m > m^* \\), the derivative is positive (pushing \\( m \\) down). At \\( m = m^* \\), half the data points are \\( \\leq m^* \\) and half \\( \\geq m^* \\), so the total subderivative includes 0, achieving optimality. Thus, \\( m^* \\) is the **median**.\n",
       "\n",
       "---\n",
       "\n",
       "### **Summary**\n",
       "- **L2 (squared error)**: Minimized by the **mean** because squaring penalizes large deviations quadratically, leading to a balance of residuals.  \n",
       "- **L1 (absolute error)**: Minimized by the **median** because absolute penalties treat errors linearly, balancing the number of residuals on either side."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q= \"Prove that minimizing the L1 error yields the median, while minimizing the L2 error yields the mean in regression.\"\n",
    "\n",
    "response = api.chat_completion(\n",
    "    prompt=q,\n",
    "    model=\"deepseek-reasoner\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1_000,\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "printmd(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To prove that minimizing the L1 error yields the median and minimizing the L2 error yields the mean in regression, we analyze the optimization problems for each case.\n",
       "\n",
       "---\n",
       "\n",
       "### **1. Minimizing the L1 error yields the median**\n",
       "\n",
       "The L1 error (absolute error) for a set of data points \\( y_1, y_2, \\dots, y_n \\) is defined as:\n",
       "\\[\n",
       "L_1(c) = \\sum_{i=1}^n |y_i - c|\n",
       "\\]\n",
       "To minimize \\( L_1(c) \\), we find the value of \\( c \\) that minimizes the sum of absolute deviations.\n",
       "\n",
       "- The derivative of \\( |y_i - c| \\) with respect to \\( c \\) is not defined at \\( c = y_i \\), but we can analyze the problem geometrically or using subgradients.\n",
       "- The sum \\( \\sum_{i=1}^n |y_i - c| \\) is minimized when \\( c \\) is the median of the data points. This is because the median balances the number of data points on either side, minimizing the total absolute deviation.\n",
       "\n",
       "**Intuition**: The median is the point where half the data lies above and half lies below, making it the optimal choice for minimizing absolute deviations.\n",
       "\n",
       "---\n",
       "\n",
       "### **2. Minimizing the L2 error yields the mean**\n",
       "\n",
       "The L2 error (squared error) for a set of data points \\( y_1, y_2, \\dots, y_n \\) is defined as:\n",
       "\\[\n",
       "L_2(c) = \\sum_{i=1}^n (y_i - c)^2\n",
       "\\]\n",
       "To minimize \\( L_2(c) \\), we take the derivative with respect to \\( c \\) and set it to zero:\n",
       "\\[\n",
       "\\frac{dL_2(c)}{dc} = \\sum_{i=1}^n 2(y_i - c)(-1) = 0\n",
       "\\]\n",
       "Simplifying:\n",
       "\\[\n",
       "\\sum_{i=1}^n (y_i - c) = 0 \\implies \\sum_{i=1}^n y_i = n c\n",
       "\\]\n",
       "Solving for \\( c \\):\n",
       "\\[\n",
       "c = \\frac{1}{n} \\sum_{i=1}^n y_i\n",
       "\\]\n",
       "This is the arithmetic mean of the data points.\n",
       "\n",
       "**Intuition**: The mean minimizes the sum of squared deviations because it balances the distances to all data points, making it the optimal choice for minimizing squared errors.\n",
       "\n",
       "---\n",
       "\n",
       "### **Conclusion**\n",
       "- Minimizing the L1 error (absolute error) yields the **median**.\n",
       "- Minimizing the L2 error (squared error) yields the **mean**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api.chat_completion(\n",
    "    prompt=q,\n",
    "    model=\"deepseek-chat\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1_000,\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "printmd(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
